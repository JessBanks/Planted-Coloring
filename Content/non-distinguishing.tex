\section{Lower Bounds Against Local Statistics SDPs}
\label{sec:lower bound}

In this section, we prove the complementary bound to Theorem [ref], namely that if every one of $\lambda_2,...,\lambda_k$ has modulus at most $1/\sqrt{d-1}$ there exists some feasible solution to the Local Path Statistics SDP for every $m \ge 1$. We can specify a pseudoexpectation completely by way of an $(nk + 1)\times(nk+1)$ positive semidefinite matrix
$$
   \begin{pmatrix} 1 & \pseudo x^T \\ \pseudo x & \pseudo x^Tx \end{pmatrix} 
    \triangleq \begin{pmatrix} 1 & l^T \\ l & X \end{pmatrix}.
$$
After first writing down the general properties required of \textit{any} quadratic pseudoexpectation satisfying $\calB_k$, we'll show that in order for $\pseudo$ to match every moment asked of it by the $\LS(2,m)$ SDP, it suffices for it to satisfy
$$
    \pseudo p_{H,S,\tau}(x,G) \simeq \expected p_{H,S,\tau}(\bx,\bG)
$$
when $(H,S,\tau)$ is a path of length $0,...,m$ with labelled endpoints. Finally, we'll construct a pseudoexpectation matching these path moments by way of some elementary properties of the non-backtracking polynomials from Section [ref].

\begin{lemma}
    The set of $\calB_k$-satisfying pseudoexpectations is parameterized by pairs $(X,l) \in \bbR^{nk\times nk}\times \bbR^{nk}$ for which
    \begin{align}
        \begin{pmatrix} 1 & l^T \\ l& X \end{pmatrix} &\succeq 0 \\
        \diag(X) &= l \\
        \Tr X_{u,u} &= e^T l = 1 & &\forall u\in[n]  \\
        X_{u,v}e &= l_u & &\forall u,v\in[n]
    \end{align}
\end{lemma}

\begin{proof}
    Recall that the set $\calB_k$ is defined by the polynomial equations
    \begin{align*}
    	\textit{Boolean} & & x_{u,i}^2 &= x_{u,i} & &\forall u\in[n] \text{ and } i\in [k] \\
    	\textit{Single Color} & & \sum_i x_{u,i} &= 1 & &\forall u\in[n] 
    \end{align*}
    That a degree-two pseudoexpectation \textit{satisfies} these constraints means 
    \begin{align*}
        \pseudo p(x)x_{u,i}^2 &= \pseudo p(x) x_{u,i} & &\forall p \,\st\,\deg p = 0 \\
        \pseudo p(x)\sum_i x_{u,i} &= \pseudo p(x) & &\forall p \,\st\,\deg p \le 1. 
    \end{align*}
   Writing $X = \pseudo x^T x$ and $l = \pseudo x$ as above, the first constraint is equivalent to $l = \diag(X)$, since the degree-zero polynomials are just constants, and we can guarantee that the second holds for every polynomial of degree at most one by requiring it on $p = 1$ and $p= ,x_{v,j}$ for all $v$ and $j$. The Lemma is simply a concise packaging of these facts, using the block notation $X = (X_{u,v})_{u,v\in[n]}$ and $l = (l_u)_{u\in [n]}$.
\end{proof}

\begin{proposition}
    It suffices to check
    $$
        \pseudo p_{H,S,\tau}(x,\bG) \simeq \expected p_{H,S,\tau}(\bx,\bG)
    $$
    in the cases (i) $(H,S,\tau)$ is a path of length $s=0,...,m$ with labelled endpoints, and (ii) when $(H,S, \tau)$ is a graph with no edges on one or two labelled vertices.
\end{proposition}

\begin{proof}
    Assume that $\pseudo$ matches all the promised moments, and let $(H,S,\tau)$ be an arbitrary partially labelled graph with $\ell$ connected components, and two distinguished vertices connected by a path of length $0 \le \le \infty$, where $s = 0$ means that the distinguished vertices, and their labels, are identical, and $s = \infty$ means that they have no path connecting them. Let's use the shorthand $(P_s,i,j)$ for the path of length $s$ with distinguished endpoints labelled $i$ and $j$ respectively. This extends naturally to the corner cases $s = 0$, when there is only one distinguished vertex and $i = j$, and $s = \infty$ when we will interpret it as two disconnected and labelled vertices.

    Our central claim will be that, as polynomials, with high probability
    \begin{equation} \label{eq:poly-apx}
        \left\| n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}}p_{P_s,i,j} - p_{H,S,\tau} \right\|_1 = o(n^\ell) \simeq_\ell 0
    \end{equation}
    where here $\|\cdot \|$ means the coefficient-wise $L^1$ norm. To get a feel for this, in the case when $H$ is connected and its two distinguished vertices are connected by a path of length $s$, $\tfrac{C_{H,S,d}}{C_{P_s,d}}$ counts the number of ways to place the remainder of $H$ in a $d$-regular tree (or locally-treelike graph), once we commit to the locations of the two distinguished vertices and the path between them. 

    Once we've shown \eqref{eq:poly-apx}, it is a standard SoS calculation that the Boolean constraint implies $|\pseudo x_{u,i}x_{v,j}| \le 1$ for every $u,v,i,j$. Thus, using the local statistic constraints from Proposition [ref], we will have
    $$
        \pseudo p_{H,S,\tau} \simeq_\ell n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}} \pseudo p_{P_s,i,j} \simeq_\ell n^\ell \pi(i)M_{i,j}^s C_{d,H,S}
    $$
    as desired. To prove \eqref{eq:poly-apx}, we need to open up the subgraph counting polynomials $p_{H,S,\tau}$ a bit more, and we'll require some notation to do this cleanly. For each $(H,S,\tau)$, write $\Phi_{H,S,u,v}$ for the set of all injective homomorphisms $H \to \bG$ such that the distinguished vertices are mapped to the vertices $u$ and $v$ of $\bG$. Then
    \begin{align*}
        \left\|n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}} p_{P_s,i,j}(x) - p_{H,S,\tau}(x)\right\|
        &= \left\|\sum_{u,v} \left(n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}}|\Phi_{P_s,u,v}| - |\Phi_{H,S,\tau}| \right) x_{u,i}x_{v,j}\right\| \\
        &= \sum_{u,v} \left|n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}}|\Phi_{P_s,u,v}| - |\Phi_{H,S,u,v}| \right|.
    \end{align*}

    Let's first consider the case when $s < \infty$, so that the two distinguished vertices in $H$ are connected by some path of length at most $m$. Under this assumption $|\Phi_{H,S,u,v}| = O(n^{\ell - 1})$. Writing $U$ for the set of $m$-good vertices---which by Lemma [ref] has size at least $n - O(\log n)$---and recalling that every vertex has at most $O(1)$ vertices in its depth-$m$ neighborhood, we can restrict the above sum to run over only $u,v \in U$ and pay a cost of no more than $O(n^{\ell - 1}\log n) = o(n^\ell)$. Thus
    \begin{align*}
        \left\| n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}} p_{P_s,i,j}(x) - p_{H,S,\tau}(x) \right\|_1 
        &= \sum_{u,v \in U} \left|n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}}|\Phi_{P_s,u,v}| - |\Phi_{H,S,u,v}| \right| + o(n^\ell).
    \end{align*}
    There are only $O(n)$ nonzero terms in the above sum, again because $u\in U$ means that there are only $O(1)$ vertices $v$ reachible via a path of length $s \le m$, and both $\Phi_{H,S,u,v}$ and $\Phi_{P_s,u,v}$ are empty if $u$ and $v$ are not connected in this way. This means we've reduced the problem to showing that
    $$
        \left|n^{\ell - 1}\frac{C_{d,H,S}}{C_{d,P_s,T}}|\Phi_{P_s,u,v}| - |\Phi_{H,S,u,v}| \right| = o(n^{\ell - 1}).
    $$

    Let's write $H = H_1 \sqcup H_2 \cdots \sqcup H_\ell$, and assume that both distinguished vertices are in $H_1$. Recall for later use that $C_{H,S,d} = C_{H_1,S,d} C_{H_2,\emptyset}\cdots C_{H_\ell,\emptyset}$. Since each of the components is of constant size, we claim that
    $$
        |\Phi_{H,S,u,v}| = |\Phi_{H_1,S,u,v}| |\Phi_{H_2}| \cdots |\Phi_{H_\ell}| + o(n^{\ell - 1})
    $$
    where by the latter terms we mean the number of occurrernces of $H_2,...,H_\ell$ respectively in $G$, without any constraints of distinguished vertices or labels. The idea is that we can choose one of the occurrences in $\Phi_{H,S,u,v}$ by first choosing where to place $(H_1,S)$ so that the vertices in $S$ map to $u$ and $v$, then choosing where to place $H_2$, etc. Since each component is of constant size, we overcount only by an additive $O(n^{\ell - 1})$ by ignoring the requirement that the connected components not collide. But $|\Phi_{H_i}| = p_{H_i,\emptyset,\emptyset}(\bx,\bG)$ for each $i=2,...,\ell$, and by Proposition each of these are $nC_{d,H_i,\emptyset} + o(n)$. Similarly, from our discussion above,
    $$
        |\Phi_{H_1,S_1,u,v}| = \frac{C_{H,S,d}}{C_{P_s,d}},
    $$
    since once we declare that the two vertices in $S$ are mapped to $m$-good vertices $u$ and $v$, our freedom in place the rest of $H_1$ is exactly the number of ways to to do so in a $d$-regular tree. But now we are done:
    $$
        |\Phi_{H,S,u,v}| = |\Phi_{H_1,S_1,u,v}||\Phi_{H_2}|\cdots |\Phi_{H_\ell}| + o(n^{\ell - 1}) = n^{\ell - 1}\frac{C_{H_1,S,d}}{C_{P_s,d}}C_{H_2}\cdots C_{H_\ell} + o(n^{\ell - 1}) = n^{\ell - 1}\frac{C_{H,S,d}}{C_{P_s,d}} + o(n^{\ell - 1}).
    $$

    We finally need to treat the case when $s = \infty$ and the distinguished vertices of $H$ are in different connected components; under this assumption $|\Phi_{H,S,u,v}| = O(n^{\ell - 2})$, since there are only $\ell - 2$ components to place once we commit to mapping the distinguished vertices to a given $u$ and $v$. Again we need to consider
    $$
        \sum_{u,v}\left|n^{\ell - 1}\frac{C_{H,S,d}}{C_{P_s,d}}|\Phi_{P_s,u,v}| - |\Phi_{H,S,u,v}|\right|,
    $$
    and again we can restrict the sum to the $m$-good vertices for the price of an additive $o(n^\ell)$, since we are omitting $O(n\log n)$ terms with magnitude $O(n^{\ell - 2})$. From this point the calculation continues more or less as it does above.
\end{proof}
    
Proposition [ref] in hand, we can now set about constructing a pseudoexpectation. We'll construct $l\in \bbR^{nk}$ and $X \in \bbR^{nk\times nk}$ so that (i) the $\calB_k$ constraints in Lemma [ref] hold, and (ii)
\begin{align*}
    \langle e, l_i \rangle &= \pi(i)n \\
    \langle X_{i,j}, \nb{s}{\bG} \rangle &\simeq \pi(i)M_{i,j}^s n \\
    \langle X_{i,j}, \bbJ \rangle &= \pi(i)\pi(j)n^2. 
\end{align*}
It will simplify things immensely to use the same change of basis as we did in Section [ref]. Namely, letting $F$ be the matrix of right eigenvectors, we will produce a pair $\check l \in \bbR^{nk}$ and $\check X \in \bbR^{nk\times nk}$ so that $l = (F^{-T} \otimes \1) \check l$ and $X = (F^{-T}\otimes \1) \check X (F^{-1} \otimes \1)$ satisfy the above conditions. Recycling the relevant calculations from Section [ref], the above moment conditions translate to
\begin{align*}
    \langle e, \check l_i \rangle &= \begin{cases} n & i = 1 \\ 0 & \text{else} \end{cases} \\
    \langle \check X_{i,j}, \nb{s}{\bG} \rangle &\simeq \lambda_i^s \|q_s\|^2_{\km} n \\
    \langle \check X_{i,j}, \bbJ \rangle &= \begin{cases} n^2 & i=j = 1 \\ 
    0 & \text{else} \end{cases}
\end{align*}

The key steps in designing $\check X$ are as follows.
%
\begin{proposition}
    For every $m$ and every $\lambda$ such that $|\lambda|^2(d-1) < 1$, there exists a polynomial $y$ nonnegative on $(-2\sqrt{d-1},2\sqrt{d-1})$ and satisfying
    $$
        \langle q_s,y \rangle_{\km} = \lambda^s\|q_s\|^2_{\km}.
    $$
\end{proposition}
\begin{proposition}
    Let $\bG \sim \Null$. If there exists a polynomial $y$ meeting the conditions of Proposition [ref] for some $\lambda \in (-1,1)$, then there exists a PSD matrix $Y(\lambda)$ for which
    \begin{align*}
        Y(\lambda)_{u,u} &= 1 & & \forall u\in[n] \\
         \langle Y(\lambda), \nb{s}{\bG} \rangle &\simeq \lambda^s \|q_s\|^2_{\km}n & & \forall s \in[m] \\
         \langle Y(\lambda),\bbJ \rangle &= 0
    \end{align*}
\end{proposition}

\noindent With these propositions in hand, define $\check X$ to be the $k\times k$ block diagonal matrix
$$
    \check X = \begin{pmatrix} 
        \bbJ & & & \\
           &Y(\lambda_2)& & \\
           & &\ddots & \\
           & & & Y(\lambda_k),
    \end{pmatrix}
$$
i.e. $\check X_{i,j} = 0$ when $i\neq j$, and the diagonal blocks are as above, and similarly let $\check l = (e,0,...,0)^T$. This way, certainly 

\begin{equation}
    \begin{pmatrix} 1 & \check l^T \\ \check l & \check X \end{pmatrix} \succeq 0  
\end{equation}
 
\noindent (by taking a Schur complement), and the three inner product conditions above are satisfied on every block. We now need to check carefully that 
$$
    \begin{pmatrix} 1 & l^T \\ l & X \end{pmatrix}
    \triangleq \begin{pmatrix} 1 & \\ & F^{-T}\otimes \1 \end{pmatrix} \begin{pmatrix} 1 & \check l^T \\ \check l & \check X \end{pmatrix} \begin{pmatrix} 1 & \\ & F^{-1}\otimes \1 \end{pmatrix}
$$
is a pseudoexpectation satisfying $\calB_k$. Equation [ref] guarantees PSD-ness, since we have multiplied a matrix and its transpose on the right and left respectively of a PSD matrix. Since $\pi$ is the first row of $F^{-1}$, we know $l_i = \pi(i)e$. On the other hand, $X$ is obtained by changing basis block-wise, the diagonal of $X$ depends only on the diagonals of $\bbJ$ and the $Y(\lambda_i)$, all of which are all ones, so
\begin{align*}
    \diag X 
    &= \diag\big((F^{-T}\otimes \1)\Diag\diag \check X (F^{-1}\otimes \1)\big) \\
    &=\diag\big((F^{-T}\otimes \1) (F^{-1}\otimes \1)\big) \\
    &= \diag\left(F^{-T}F^{-1} \otimes \1\right) \\
    &= \diag\left(\Diag\pi \otimes \1\right) \\
    &= (\pi(1)e,...,\pi(k)e)
\end{align*}
as desired. Similarly, because $\check X$ is diagonal, $\check X_{u,u} = \1$, and
$$
    \Tr X_{u,u} = \Tr F^{-T}\check X_{u,u} F^{-1} = \Tr F^{-T}F^{-1} = \Tr \Diag \pi = 1.
$$
Finally, the top row of each $\check X_{u,v}$ is the vector $e_1^T$, so
$$
    X_{u,v}e = F^{-T}\check X_{u,v}F^{-1}e = F^{-T}\check X_{u,v}e_1 = F^{-T}e_1 = \pi = l_u.
$$
This completes the construction of our pseudoexpectation.

\begin{remark}
    By correctly setting the $\epsilon$ from Proposition [ref] and the global error tolerance $\delta$, we can once again choose $(\epsilon,\delta) = (a,b)$ with the property that whenever the $\LS(2,m)$ SDP is feasible $\delta = b$, it is feasible as well with $\delta = 2b$. Thus every one of the $\simeq$ constraints---i.e. those that depend on the observed graph $\bG$---are satisfied with slack $\Omega(n)$.
\end{remark}

\begin{proof}[Proof of Proposition 6.1]
    Such a polynomial $y$ is exactly of the form
    $$
        y = \sum_{s = 0}^m \lambda^s q_s + \text{terms with larger $q_s$'s}.
    $$
    We will use the extremely simple construction of letting the coefficients on the terms $q_{m+1},q_{m+1}, \cdots$ \emph{also} be powers of $\lambda$. The idea here is that, whenever $|\lambda|^2(d-1) < 1$, this series converges to a positive function on $(-2\sqrt{d-1},2\sqrt{d-1})$, so by taking a long enough initial segment, we can get a positive approximant. 

    In particular, let $p \gg m$ be even, and set
    $$
        y = \sum_{s = 0}^p \lambda^s q_s.
    $$
    It is a standard calculation, employing the recurrence relation on the polynomials $q_s$, that
    $$
        y(x) = \frac{1 - \lambda^2 + \lambda^{p+2}(d-1)q_p(x) - \lambda^{p+1}q_{p+1}(x)}{(d-1)\lambda^2 - \lambda x + 1}.
    $$
    One an quickly verify that
    $$
        \frac{1 - \lambda^2}{(d-1)\lambda^2 - \lambda x + 1} > 0 \qquad \forall |x| \le 2\sqrt{d-1},
    $$
    so all we need to verify is that $\lambda^2(d-1) < 1$ ensures $\lambda^{p+2}(d-1)q_p - \lambda^{p+1}q_{p+1} \to_p 0$. This follows immediately from Lemma [ref], as $|q_p| \le 2p\sqrt{d(d-1)^p}$.
\end{proof}

\begin{proof}[Proof of Proposition 6.***]
    Let $y$ be the polynomial guaranteed in the theorem statement; our strategy will be to modify the matrix $y(A_{\bG})$. First note that by expanding $y$ in the $q_s$ basis, we have
    $$
        y(A_{\bG}) = \sum_{s = 0}^m q_s(A_{\bG})\lambda^s + \cdots = \sum_{s=0}^m \nb{s}{\bG}\lambda^s + \cdots,
    $$
    so it is clear that $y(A_{\bG})$ satisfies the affine constraints against the $\nb{s}{\bG}$ matrices. Moreover, as $y$ is strictly positive on $[-2\sqrt{d-1},2\sqrt{d-1}]$, it is (by continuity) nonnegative on a constant size fattening of this interval, and by Friedman's theorem the spectrum of $A_{\bG}$ other than the eignevalue at $d$ is contained w.h.p. in such a set. Thus $y(A_{\bG})$ is positive, except perhaps the eigenvalue $y(d)$, which we will fix in a moment.
    
    However, $y(A_{\bG})$ does not have the right inner product with the all ones matrix, and---unless $2\deg y + 1 < \girth(G)$---its diagonal entries need not be ones. Our corrective to these issues will exploit two fortunate facts. First, those diagonal entries different from one exactly correspond to the $(2\deg y + 1$)-bad vertices in $G$; from Lemma [ref] we know that there are at most $O(\log n)$ of these. Second, as $y$ is a scalar polynomial and $\bbJ$ commutes with $A_{\bG}$,
    $$
        \langle y (A_{\bG}),\bbJ/n \rangle = y(d) = O_n(1).
    $$
    
    Thus we can correct the inner product with $\bbJ$, and at the same time resolve the possible negativity of the eigenvalue $y(d)$, by passing to
    \begin{align*}
         \tilde Y(\lambda) 
         &= \frac{1}{1 - y(d)/n} \left(\1 - \bbJ/n\right) y(A_{\bG})\left(\1 - \bbJ/n\right) \\
         &= \frac{1}{1 - y(d)/n} \big(y(A_{\bG}) - y(d)\bbJ/n\big);
    \end{align*}
    since $\langle \nb{\bG}{s},\bbJ/n\rangle = q_s(d) = O(1)$ the result will still satisfy the inner product constraints with the matrices $\nb{\bG}{s}$ up to an additive $\pm \delta n$. This new matrix is certainly PSD, for instance by writing out $y(A_{\bG})$ in its eigenvalue basis, and observing that left and right multiplication by $(\1 - \bbJ/n)$ simply projects away the eigenspace of the $y(d)$ eigenvalue. Thus we can write the $\tilde Y(\lambda)_{u,v} = \alpha_u^T\alpha_v$ for some vectors $\alpha_1,...,\alpha_n \in \bbR^n$. The scale factor we applied above makes sure that for every $u$ that is not $(2\deg y + 1)$-bad, $\|\alpha_u\| = 1$, and being orthogonal to the all-ones matrix is equivalent to $\sum_u \alpha_u = 0$.
    
    The remaining diagonal elements are at worst some constant $C$ dependent on $d$ and $y$, since the diagonal entries of each $\nb{s}{\bG}$ are all $O(1)$. Thus, writing $U$ for the set of $(2\deg y + 1)$-bad vertices, we know
    $$
        \left\|\sum_{u \notin U} \alpha_u \right\| = \left\|\sum_{u\in U} \alpha_u \right\| \le C\log n
    $$
    It is clear that by enlarging $U$ to a set $U'$ of size at most $C\log n$, we can choose a collection of unit vectors $\beta_u$ for each $u \in U'$ so that $$
        \sum_{u \in U'} \beta_u = \sum_{u\notin U'} \alpha_u. 
    $$
    Our final matrix $Y(\lambda)$ will be the Gram matrix of these new $\beta$ and remaining $\alpha$ vectors. We must finally check that the affine constraints against the $\nb{s}{\bG}$ matrices are still approximately satisfied. However, even starting from a bad vertex, there are at most a constant number of vertices within $s$ steps of it, and at most a constant number of non-backtracking walks to any such vertex. Thus
    \begin{align*}
        \left|\langle Y(\lambda),\nb{s}{\bG} \rangle - \langle \tilde Y(\lambda),\nb{s}{\bG} \rangle \right| &= \left| 2\sum_{u\in U',v\notin U'} (\nb{s}{\bG})_{u,v}\alpha_u^T(\alpha_v - \beta_v) + \sum_{u,v\in U'} (\nb{s}{\bG})_{u,u}\left(\|\alpha_u\| - \|\beta_u\|\right)\right| \\
        &= O(\log n)
    \end{align*}
    where we have used that $\max_u \|\alpha_u\| = O(1)$ and broken up both summations by first enumerating the $O(\log n)$ vertices in $U'$ and then the at most $O(1)$ vertices in its depth $s$ neighborhood.
\end{proof}

